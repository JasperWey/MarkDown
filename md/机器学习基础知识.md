### 机器学习基础知识



反向传播 backpropagation:**BP算法正是用来求解这种多层复合函数的所有变量的偏导数的利器**。

如何计算梯度呢？ backpropogation



bp算法对于深度学习的意义

反向传播伪代码：

![backpropergation](/home/jiangwei/桌面/md/images/backpropergation.jpg)


### 离开环境



#GPU

## 迁移学习



##读论文，重现其中的研究

生成对抗网络GAN



SVM对于线性不可分的情况，使用核函数kernel，将特征向量从低维转到高维空间，寻找超平面进行分类。常见的核函数：

1. 多项式核
2. 傅立叶核
3. B样条核
4. Sigmod核
5. 高斯径向基核

## 随机森林

抓住两个点：

1. 森林：指的是由多棵决策树组成的
2. 每棵树用不同的方式定义相似性，预测不同的值。这样可以允许每一棵树发现数据的一些不同的特性
3. 随机：包括两个方面，1）样本采集是随机的  2）样本的特征采集也是随机的，这样可以有效的防止overfitting

最终的结果由这些决策树投票得出

####优缺点

#####优点

1、 在当前的很多数据集上，相对其他算法有着很大的优势，表现良好

2、它能够处理很高维度（feature很多）的数据，并且不用做特征选择

​        PS：特征子集是随机选择的

3、在训练完后，它能够给出哪些feature比较重要

​        PS：http://blog.csdn.net/keepreder/article/details/47277517

4、在创建随机森林的时候，对generlization error使用的是无偏估计，模型泛化能力强

5、训练速度快，容易做成并行化方法

​       PS：训练时树与树之间是相互独立的

6、 在训练过程中，能够检测到feature间的互相影响

7、 实现比较简单

8、 对于不平衡的数据集来说，它可以平衡误差。

9、如果有很大一部分的特征遗失，仍可以维持准确度。

#####缺点：

1. 随机森林已经被证明在某些**噪音较大**的分类或回归问题上会过拟
2. 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。


## 数据缺失处理方法

#### 1. 删除  （数据量缺失严重，且数据单元之间无强关联）

#### 2. 不作处理 （设计算法时，考虑到了缺失情况，算法能够在该情况下，仍能够挖掘数据信息）

#### 3. 填充

- [ ] 数据缺失原因

      1. 如果是missing at random,则考虑使用EM或者imputation来处理
      2. 如果不是，就要考虑是不是数据采集方式有缺陷，尝试改进数据采集方式

      #### 填充方法

      1. 值替代   （平均值  中位数  众数  随机值等），效果一般
      2. 用其他变量做预测模型来算出缺失变量。效果比方法1略好。有一个根本缺陷，如果其他变量和缺失变量无关，则预测的结果无意义。如果预测结果相当准确，则又说明这个变量是没必要加入建模的。一般情况下，介于两者之间。
      3. 最精确的做法，把变量映射到高维空间。比如性别，有男、女、缺失三种情况，则映射成3个变量：是否男、是否女、是否缺失。连续型变量也可以这样处理。比如Google、百度的CTR预估模型，预处理时会把所有变量都这样处理，达到几亿维。这样做的好处是完整保留了原始数据的全部信息、不用考虑缺失值、不用考虑线性不可分之类的问题。缺点是计算量大大提升。 而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差。




###另一位回答者

看缺失值占比， 本来就是个估计数，要是就缺少一点点可以pass。 占比一定，如果有时间序列特征，就利用已有的数据，利用时间序列模型计算一个数字填充；如果是普通的， 看数字的分布特征，简单就用正态分布，做一个不影响整体估计数的计算出缺失值； 根据数据库特征补缺失值。 市面上有专门的缺失值处理书，整整一本都在搞这个东西。 我觉得缺失值的填补的精髓就是不能影响整体的估计，它就只能是占个位置，让程序运行下去，让已有的别的属性的值能发挥作用。 填补数与已有的数的分布、特征应符合，不能因为这些数字的变更导致估计值变坏。

核心思想就是：填充的数据不会影响到整体的估计，填充的值，让程序完成的跑下去，让已有的别的属性发挥作用

### 贝叶斯定理（理解）

上式中的 Pants 和 Boy/Girl 可以指代一切东西，所以其一般形式就是：

**P(B|A) = P(A|B) \* P(B) / [P(A|B) * P(B) + P(A|~B) * P(~B) ]**

收缩起来就是：

**P(B|A) = P(AB) / P(A)**

其实这个就等于：

**P(B|A) \* P(A) = P(AB)**



## 极大似然估计

使用实验结果来估测样本模型

比如，袋子里红蓝两种球，数量未知。又放回的拿去10次，蓝球8次，红球2次，就可以知道样本模型为，红蓝求比例为1:4

就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！ 换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。

# KMeans

### 1.K-Means算法K值如何选择？

《大数据》中提到：给定一个合适的类簇指标，比如平均半径或直径，只要我们假设的类簇的数目等于或者高于真实的类簇的数目时，该指标上升会很缓慢，而一旦试图得到少于真实数目的类簇时，该指标会急剧上升。

- 簇的直径是指簇内任意两点之间的最大距离。
- 簇的半径是指簇内所有点到簇中心距离的最大值。

### 2. 如何优化K-Means算法搜索的时间复杂度？

可以使用K-D树来缩短最近邻的搜索时间（NN算法都可以使用K-D树来优化时间复杂度）。

### 3. 如何确定K个簇的初始质心？

1) 选择批次距离尽可能远的K个点

首先随机选择一个点作为第一个初始类簇中心点，然后选择距离该点最远的那个点作为第二个初始类簇中心点，然后再选择距离前两个点的最近距离最大的点作为第三个初始类簇的中心点，以此类推，直至选出K个初始类簇中心点。

2) 选用层次聚类或者Canopy算法进行初始聚类，然后利用这些类簇的中心点作为KMeans算法初始类簇中心点。

聚类扩展：密度聚类、层次聚类。





### 方差与偏差

1. 偏差是反应测试值与真实值之间的差距
2. 方差反应本集合中数据之间的离散程度，各个数据与期望值的距离。



### 正则化

作用就是选择经验风险和模型复杂度同时较小的模型

惩罚函数 L2 L1等



## 批量梯度，随机梯度算法



# Adam

__概念__:

​	一般，随机梯度下降算法会保持单一的学习率来更新权重。学习率在训练过程中不会改变。而Adam算法通过计算梯度的一阶和二阶矩估计，为不同的参数设计独立的__自适应学习率__。

优势:

Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。具体来说，算法计算了梯度的指数移动均值（exponential moving average），超参数 beta1 和 beta2 控制了这些移动均值的衰减率。

![v2-6be80dc242aeac7a67371dce0a3fba82_hd](/home/jiangwei/桌面/md/images/v2-6be80dc242aeac7a67371dce0a3fba82_hd.jpg)

## ??一阶矩  二阶矩





## 熟练使用Numpy库





## Q为什么EM(集成方法)会优于单个分类器？

1. 平均了bias
2. 减少了variance
3. 不太可能过拟合



## 数据的稀疏性

## 压缩算法

数据之间可能存在相关性，从而增加了问题的复杂性。而对问题的分析往往是独立的，不是综合的。盲目的减少变量指标，会损失很多信息，容易产生错误的结论





## 数据扩充

1. 旋转平移

2. 缩放

3. 色彩抖动等

   开源工具包：https://github.com/aleju/imgaug







## Q人脸识别的主流算法



##梯度爆炸？

【解析】误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。
在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。
网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。

###梯度爆炸会引发什么问题？

【解析】在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。
梯度爆炸导致学习过程不稳定。—《深度学习》，2016.
在循环神经网络中，梯度爆炸会导致网络不稳定，无法利用训练数据学习，最好的结果是网络无法学习长的输入序列数据。

###如何确定是否出现梯度爆炸？

【解析】训练过程中出现梯度爆炸会伴随一些细微的信号，如：
模型无法从训练数据中获得更新（如低损失）。
模型不稳定，导致更新过程中的损失出现显著变化。
训练过程中，模型损失变成 NaN。
如果你发现这些问题，那么你需要仔细查看是否出现梯度爆炸问题。
以下是一些稍微明显一点的信号，有助于确认是否出现梯度爆炸问题。
训练过程中模型梯度快速变大。
训练过程中模型权重变成 NaN 值。
训练过程中，每个节点和层的误差梯度值持续超过 1.0。

###如何修复梯度爆炸问题？

【解析】有很多方法可以解决梯度爆炸问题，本节列举了一些最佳实验方法。

1.  重新设计网络模型

在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。
使用更小的批尺寸对网络训练也有好处。
在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。

2. 使用 ReLU 激活函数

在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。
使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。

3.  使用长短期记忆网络

在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。
使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。
采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。

4. 使用梯度截断（Gradient Clipping）

在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。
处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。
 ——《Neural Network Methods in Natural Language Processing》，2017.
具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。
梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。
​     ——《深度学习》，2016.
在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。
默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/。

5. 使用权重正则化（Weight Regularization）

如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。
对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。
——On the difficulty of training recurrent neural networks，2013.
在 Keras 深度学习库中，你可以通过在层上设置 kernel_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。



## 残差网络(residual network)待补充

高速公路网络





### 正负样本不均衡问题

- 数据层面（努力平衡样本）
  - 欠采样，减少大样本数量。
    - 缺点：有用信息丢失。
    - 改进：将大样本分N类，每类M个样本，与小样本E组成M+E的数据集进行模型训练
  - 过采样，复制小样本数量。
    - 缺点：简单的复制，容易造成过拟合
    - 改进：基于现有样本，构造新的小样本
- 算法模型层面
  - 增加小样本的话语权和，即增加权重





## 降维(feature extraction, feature selection)

### PCA(主成分分析)

简单来说，就是通过正交变换，将可能存在相关性变量的观测值转换为线性不相关的变量值，转换后的变量就是所谓的主成分分析。

https://www.jianshu.com/p/673d4fe72362?utm_campaign=maleskine&utm_content=note&utm_medium=pc_all_hots&utm_source=recommendation

http://blog.csdn.net/zhongkelee/article/details/44064401

1. 算均值，并去中心化(变量减去均值)
2. 算出协方差矩阵
3. 算出协方差矩阵的特征向量和特征值
4. 将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。
5. 将样本点投影到选取的特征向量上。

```
PCA的算法步骤：
设有 m 条 n 维数据。

1）将原始数据按列组成 n 行 m 列矩阵 X

2）将 X 的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值

3）求出协方差矩阵 C=1/mXX𝖳

4）求出协方差矩阵的特征值及对应的特征向量

5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P

6）Y=PX 即为降维到 k 维后的数据

作者：不会停的蜗牛
链接：https://www.jianshu.com/p/673d4fe72362
來源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
```

####PCA的数学原理？

###协方差

表示数据集之间的联系，如果一个值随着另一个值增大而增大，则协方差大于0.反之亦然

#### 相关性

取出了量纲的协方差

####SVD(奇异值分解)



### 优化目标

降维问题的**优化目标**：
将一组 N 维向量降为 K 维，目标是选择 K 个单位正交基，使得原始数据**变换到这组基上后**，各字段两两间协方差为0，并且字段的方差则尽可能大（即在正交的约束下，取最大的K个方差）。





### 剪枝(待补充)

在减少模型复杂度的同时，有效防止过拟合，提升模型的泛化能力





## Adaboost分类器

**思想**： 多个弱分类器，经过权重组合，形成adaboost分类器。

**关键点**：

- 两个权重
  - 样本权重，会增加分错样本的权重，以提高下次分类的重要性
  - 分类器权重，减少错误率高的分类器的权重，增加错误率低的分类器的权重
- 何时结束
  - 当样本分类不在发生错误为止
- 缺点
  - 对噪声和异常数据非常敏感
- 优点
  - 准确率大幅提高
  - 分类速度快，基本不用调参数
  - 过拟合情况几乎不会出现
  - 构造分类器时，有多种方法可以使用
  - 方法简单，容易理解，不用做特征分类



# SVM分类器

对于线性不可分的场景，可以将数据从低维映射到高维来。但是维度越高，计算越复杂。

###为什么要引入核函数？

因此，引入了核函数的概念，将特征由低维转到高维，但是计算依然在低维进行，避免的了在高维上直接进行复杂的数学计算。



SVM在针对海量数据的分类时，需要很多的时间用于训练模型，此外，SVM也无法有效处理包含太多噪声的数据



## 二次规划问题

### KKT



###VC dimnesion





##如何预测用户的流失

# (占坑)强化学习

#(占坑)进化学习



# 推荐系统之协同过滤

#### 基于用户的CF和基于物品的CF优缺点及使用场景

- 计算复杂度
  - Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。但我们往往忽略了这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。
- 适用场景
  - 在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。
  - 相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。
- 推荐多样性和精度
  - 研究推荐引擎的学者们在相同的数据集合上分别用 User CF 和 Item CF 计算推荐结果，发现推荐列表中，只有 50% 是一样的，还有 50% 完全不同。但是这两个算法确有相似的精度，所以可以说，这两个算法是很互补的。
  - 关于推荐的多样性，有两种度量方法：
    - 第一种度量方法是从单个用户的角度度量，就是说给定一个用户，查看系统给出的推荐列表是否多样，也就是要比较推荐列表中的物品之间两两的相似度，不难想到，对这种度量方法，Item CF 的多样性显然不如 User CF 的好，因为 Item CF 的推荐就是和以前看的东西最相似的。
    - 第二种度量方法是考虑系统的多样性，也被称为覆盖率 (Coverage)，它是指一个推荐系统是否能够提供给所有用户丰富的选择。在这种指标下，Item CF 的多样性要远远好于 User CF, 因为 User CF 总是倾向于推荐热门的，从另一个侧面看，也就是说，Item CF 的推荐有很好的新颖性，很擅长推荐长尾里的物品。所以，尽管大多数情况，Item CF 的精度略小于 User CF， 但如果考虑多样性，Item CF 却比 User CF 好很多。
  - 其实对这类系统的最好选择是，如果系统给这个用户推荐 30 个物品，既不是每个领域挑选 10 个最热门的给他，也不是推荐 30 个 A 领域的给他，而是比如推荐 15 个 A 领域的给他，剩下的 15 个从 B,C 中选择。所以结合 User CF 和 Item CF 是最优的选择，结合的基本原则就是当采用 Item CF 导致系统对个人推荐的多样性不足时，我们通过加入 User CF 增加个人推荐的多样性，从而提高精度，而当因为采用 User CF 而使系统的整体多样性不足时，我们可以通过加入 Item CF 增加整体的多样性，同样同样可以提高推荐的精度。
- 用户对推荐算法的适应度
  - 前面我们大部分都是从推荐引擎的角度考虑哪个算法更优，但其实我们更多的应该考虑作为推荐引擎的最终使用者 -- 应用用户对推荐算法的适应度。
  - 对于 User CF，推荐的原则是假设用户会喜欢那些和他有相同喜好的用户喜欢的东西，但如果一个用户没有相同喜好的朋友，那 User CF 的算法的效果就会很差，所以一个用户对的 CF 算法的适应度是和他有多少共同喜好用户成正比的。
  - Item CF 算法也有一个基本假设，就是用户会喜欢和他以前喜欢的东西相似的东西，那么我们可以计算一个用户喜欢的物品的自相似度。一个用户喜欢物品的自相似度大，就说明他喜欢的东西都是比较相似的，也就是说他比较符合 Item CF 方法的基本假设，那么他对 Item CF 的适应度自然比较好；反之，如果自相似度小，就说明这个用户的喜好习惯并不满足 Item CF 方法的基本假设，那么对于这种用户，用 Item CF 方法做出好的推荐的可能性非常低。

### 协同过滤的缺点

**协同过滤的缺点是:**

　　（1）用户对商品的评价非常稀疏，这样基于用户的评价所得到的用户间的相似性可能不准确（即稀疏性问题）;

　　（2）随着用户和商品的增多，系统的性能会越来越低;

　　（3）如果从来没有用户对某一商品加以评价，则这个商品就不可能被推荐（即最初评价问题）。


